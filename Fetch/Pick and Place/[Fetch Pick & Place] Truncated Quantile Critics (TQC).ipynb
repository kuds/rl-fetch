{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNBGsrWWl7yT2fuFwVF3DRL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/rl-fetch/blob/main/Fetch/Pick%20and%20Place/%5BFetch%20Pick%20%26%20Place%5D%20Truncated%20Quantile%20Critics%20(TQC).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Truncated Quantile Critics (TQC)"
      ],
      "metadata": {
        "id": "1tAm2TLSF7LL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mujoco\n",
        "\n",
        "# Set up GPU rendering.\n",
        "from google.colab import files\n",
        "import distutils.util\n",
        "import os\n",
        "import subprocess\n",
        "if subprocess.run('nvidia-smi').returncode:\n",
        "  raise RuntimeError(\n",
        "      'Cannot communicate with GPU. '\n",
        "      'Make sure you are using a GPU Colab runtime. '\n",
        "      'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
        "# This is usually installed as part of an Nvidia driver package, but the Colab\n",
        "# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
        "# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
        "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "    f.write(\"\"\"{\n",
        "    \"file_format_version\" : \"1.0.0\",\n",
        "    \"ICD\" : {\n",
        "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "    }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
        "print('Setting environment variable to use GPU rendering:')\n",
        "%env MUJOCO_GL=egl\n",
        "\n",
        "# Check if installation was succesful.\n",
        "try:\n",
        "  print('Checking that the installation succeeded:')\n",
        "  import mujoco\n",
        "  mujoco.MjModel.from_xml_string('<mujoco/>')\n",
        "except Exception as e:\n",
        "  raise e from RuntimeError(\n",
        "      'Something went wrong during installation. Check the shell output above '\n",
        "      'for more information.\\n'\n",
        "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "\n",
        "print('Installation successful.')\n",
        "\n",
        "# Other imports and helper functions\n",
        "import time\n",
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "# Graphics and plotting.\n",
        "print('Installing mediapy:')\n",
        "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
        "!pip install -q mediapy\n",
        "import mediapy as media\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# More legible printing from numpy.\n",
        "np.set_printoptions(precision=3, suppress=True, linewidth=100)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "dVyqLLRYmImN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3 sb3-contrib"
      ],
      "metadata": {
        "id": "YF_7IareVN3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjg_WKYzcEYe"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium-robotics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import datetime\n",
        "import platform\n",
        "from importlib.metadata import version\n",
        "import gymnasium\n",
        "import os\n",
        "import numpy\n",
        "import matplotlib\n",
        "import matplotlib.pyplot\n",
        "import gymnasium_robotics\n",
        "import google.colab\n",
        "from sb3_contrib import TQC\n",
        "from stable_baselines3.her import HerReplayBuffer\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
        "from stable_baselines3.her.goal_selection_strategy import GoalSelectionStrategy"
      ],
      "metadata": {
        "id": "FJyALd5ycgbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Python Version: {platform.python_version()}\")\n",
        "print(f\"Torch Version: {version('torch')}\")\n",
        "print(f\"Is Cuda Available: {torch.cuda.is_available()}\")\n",
        "print(f\"Cuda Version: {torch.version.cuda}\")\n",
        "print(f\"Mujoco Version: {version('mujoco')}\")\n",
        "print(f\"Gymnasium Version: {version('gymnasium')}\")\n",
        "print(f\"Gymnasium Robotics Version: {version('gymnasium_robotics')}\")\n",
        "print(f\"Numpy Version: {version('numpy')}\")\n",
        "print(f\"Stable Baselines3 Version: {version('stable_baselines3')}\")\n",
        "print(f\"SB3-Contrib Version: {version('sb3_contrib')}\")"
      ],
      "metadata": {
        "id": "08dVaplqcafY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register Gymansium Robotics Environment\n",
        "gymnasium.register_envs(gymnasium_robotics)"
      ],
      "metadata": {
        "id": "Qq0Pkc-1cH9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rl_type = \"TQC\"\n",
        "env_str = \"FetchPickAndPlaceDense-v4\"\n",
        "name_prefix = f\"fetch_pick_and_place _{rl_type}\".lower()\n",
        "log_dir = \"./logs/{}\".format(env_str)"
      ],
      "metadata": {
        "id": "IQcVzVmoNeGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gymnasium.make(env_str)\n",
        "print(\"Observation Space Size: \", env.observation_space)\n",
        "print('Actions Space: ', env.action_space)\n",
        "env.close()"
      ],
      "metadata": {
        "id": "tYRDXXD7eG_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/tqc.yml\n",
        "# FetchPush-v1: &her-defaults\n",
        "#   n_timesteps: !!float 1e6\n",
        "#   policy: 'MultiInputPolicy'\n",
        "#   buffer_size: 1000000\n",
        "#   batch_size: 2048\n",
        "#   gamma: 0.95\n",
        "#   learning_rate: !!float 1e-3\n",
        "#   tau: 0.05\n",
        "#   replay_buffer_class: HerReplayBuffer\n",
        "#   replay_buffer_kwargs: \"dict(\n",
        "#     goal_selection_strategy='future',\n",
        "#     n_sampled_goal=4,\n",
        "#   )\"\n",
        "#   policy_kwargs: \"dict(net_arch=[512, 512, 512], n_critics=2)\""
      ],
      "metadata": {
        "id": "XKMnLCR9Jvqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Training Fetch Push environment\n",
        "env = make_vec_env(env_str, n_envs=4)\n",
        "\n",
        "# Create Evaluation Fetch Push environment\n",
        "env_val = make_vec_env(env_str, n_envs=1)\n",
        "\n",
        "# Create Evaluation Callback\n",
        "# eval_freq - can cause learning instability if set to low\n",
        "eval_callback = EvalCallback(env_val,\n",
        "                             best_model_save_path=log_dir,\n",
        "                             log_path=log_dir,\n",
        "                             eval_freq=50_000,\n",
        "                             render=False,\n",
        "                             deterministic=True,\n",
        "                             n_eval_episodes=20)\n",
        "\n",
        "# Instantiate the agent\n",
        "model = TQC(\"MultiInputPolicy\", env,\n",
        "            verbose=0,\n",
        "            learning_starts=1000,  # Set to a value >= max episode length\n",
        "            buffer_size=1_000_000,\n",
        "            tau=0.05,\n",
        "            learning_rate=1e-3,\n",
        "            gamma=0.95,\n",
        "            batch_size=2048,\n",
        "            policy_kwargs=dict(net_arch=[512, 512, 512], n_critics=2),\n",
        "            replay_buffer_class=HerReplayBuffer,\n",
        "            replay_buffer_kwargs=dict(\n",
        "                goal_selection_strategy=GoalSelectionStrategy.FUTURE,\n",
        "                n_sampled_goal=4,\n",
        "            ),\n",
        "            tensorboard_log=os.path.join(log_dir, \"tensorboard\"))\n",
        "\n",
        "# Train the agent with the callback\n",
        "model.learn(total_timesteps=1_000_000,\n",
        "            callback=eval_callback,\n",
        "            progress_bar=True)\n",
        "\n",
        "# Save the model\n",
        "model.save(os.path.join(log_dir, \"final_model\"))\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20)\n",
        "print(f\"Final Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n",
        "env_val.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "id": "fD9yhkSXGSXN",
        "outputId": "aa31c0ac-4899-4579-d1e8-a45b0f9dbf67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[35m   4%\u001b[0m \u001b[38;2;249;38;114m━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37,784/1,000,000 \u001b[0m [ \u001b[33m0:06:00\u001b[0m < \u001b[36m2:36:40\u001b[0m , \u001b[31m102 it/s\u001b[0m ]\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">   4%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">37,784/1,000,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:06:00</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">2:36:40</span> , <span style=\"color: #800000; text-decoration-color: #800000\">102 it/s</span> ]\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Evaluation Fetch Push environment\n",
        "env = make_vec_env(env_str, n_envs=1, seed=0)\n",
        "\n",
        "# Load the best model\n",
        "best_model_path = os.path.join(log_dir, \"best_model.zip\")\n",
        "best_model = TQC.load(best_model_path, env=env)\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(best_model, env, n_eval_episodes=20)\n",
        "print(f\"Best Model - Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "# Record video of the best model playing Fetch Push\n",
        "env = VecVideoRecorder(env, \"./videos/\",\n",
        "                       video_length=10000,\n",
        "                       record_video_trigger=lambda x: x == 0,\n",
        "                       name_prefix=\"best_model_{}\".format(name_prefix))\n",
        "\n",
        "n_sims = 5\n",
        "obs = env.reset()\n",
        "for _ in range(10000):\n",
        "    action, _states = best_model.predict(obs)\n",
        "    obs, rewards, dones, info = env.step(action)\n",
        "    env.render()\n",
        "    if dones:\n",
        "        n_sims = n_sims - 1\n",
        "        if n_sims == 0:\n",
        "            break\n",
        "        obs = env.reset()\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "pYb_YoK_GSgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the evaluations.npz file\n",
        "data = numpy.load(os.path.join(log_dir, \"evaluations.npz\"))\n",
        "\n",
        "# Extract the relevant data\n",
        "timesteps = data['timesteps']\n",
        "results = data['results']\n",
        "\n",
        "# Calculate the mean and standard deviation of the results\n",
        "mean_results = numpy.mean(results, axis=1)\n",
        "std_results = numpy.std(results, axis=1)\n",
        "\n",
        "# Plot the results\n",
        "matplotlib.pyplot.figure()\n",
        "matplotlib.pyplot.plot(timesteps, mean_results)\n",
        "matplotlib.pyplot.fill_between(timesteps,\n",
        "                               mean_results - std_results,\n",
        "                               mean_results + std_results,\n",
        "                               alpha=0.3)\n",
        "\n",
        "matplotlib.pyplot.xlabel('Timesteps')\n",
        "matplotlib.pyplot.ylabel('Mean Reward')\n",
        "matplotlib.pyplot.title(f\"{rl_type} Performance on {env_str}\")\n",
        "matplotlib.pyplot.show()"
      ],
      "metadata": {
        "id": "wGaRcDh3MA8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KjPzfbPGI8f1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}